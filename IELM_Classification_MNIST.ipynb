{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ClassificationMNIST.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "biHlFkWLYDE-"
      },
      "source": [
        "# importing necessary libraries to run the model\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import random\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sToMhzNwWC3-"
      },
      "source": [
        "# defining necessary functions \n",
        "\n",
        "# defining a sigmoid function as the activation function\n",
        "def _sigmoid(x):\n",
        "    return 1. / (1. + np.exp(-x))\n",
        "\n",
        "# defining a fourier function as the activation function\n",
        "def _fourier(x):\n",
        "    return np.sin(x)\n",
        "\n",
        "# initialising a simple identity function for the value inputed\n",
        "def _identity(x):\n",
        "    return x\n",
        "\n",
        "# initialising a function to compute the activation of the model, sigmoid is used for now. Fourier and Hard limit can be used as well\n",
        "def activation_fn_compute(name):\n",
        "    return {\n",
        "        'sigmoid': _sigmoid,\n",
        "        'fourier': _fourier,\n",
        "    }[name]\n",
        "\n",
        "# a function to compute mean square error\n",
        "def _mean_squared_error(y, pred):\n",
        "    return 0.5 * np.mean((y - pred) ** 2)\n",
        "\n",
        "# intialising a function to compute the loss of the model, mse is used for now\n",
        "def loss_compute(name):\n",
        "    return {\n",
        "        'mse': _mean_squared_error,\n",
        "    }[name]\n",
        "\n",
        "# Incremental Extreme machine learning class is initialised so that model can be run\n",
        "class IELM:\n",
        "  \n",
        "  # defining a function to initialise all the necessary variables\n",
        "  def __init__(self, input_nodes, hidden_nodes, output_nodes, activation='sigmoid',\n",
        "                loss='mse', beta_init=None, w_init=None, bias_init=None):\n",
        "    # declaring the values for the input, hidden and output nodes for the model\n",
        "    # w is the weight vector\n",
        "      self._input_nodes = input_nodes\n",
        "      self._hidden_nodes = 1\n",
        "      self._output_nodes = output_nodes\n",
        "\n",
        "    # declaring activation and loss for the model\n",
        "      self._activation = activation_fn_compute(activation)\n",
        "      self._loss = loss_compute(loss)\n",
        "\n",
        "    # computing alpha value from input and hidden nodes\n",
        "      self._w = np.random.uniform(-1, 1, size=(self._input_nodes, self._hidden_nodes))\n",
        "    \n",
        "    # computing bias value from hidden nodes\n",
        "      self._bias = np.zeros(shape=(self._hidden_nodes,))\n",
        "\n",
        "    # computing beta value from hidden and output nodes\n",
        "      self._beta = np.random.uniform(-1., 1., size=(self._hidden_nodes, self._output_nodes))\n",
        "\n",
        "  def sigmoid(self, x):\n",
        "      return 1. / (1. + np.exp(-x))\n",
        "\n",
        "  # function to place them in lists\n",
        "  def predict(self, X):\n",
        "      return list(self(X))\n",
        "\n",
        "  # a function to run the exact model which takes parameters \n",
        "  def fit(self, X, Y, LMax, display_time=False):\n",
        "      \n",
        "      # computing the value with sigmoid activation function \n",
        "      # H is the hidden layer output matrix\n",
        "      H = self.sigmoid(X.dot(self._w))\n",
        "\n",
        "      # pseudoinverse of the hidden layer matrix\n",
        "      H_pinv = np.linalg.pinv(H)\n",
        "\n",
        "      # beta gets updated\n",
        "      self._beta = H_pinv.dot(Y)\n",
        "\n",
        "      # looping it within the range of LMax which will be initialised later\n",
        "      for i in range(2,LMax):\n",
        "        # initialising random for beta and w(alpha)\n",
        "          beta_random = np.random.uniform(-1.,1.,size=(1, self._output_nodes))\n",
        "          alpha_random = np.random.uniform(-1.,1.,size=(self._input_nodes, 1))\n",
        "          self._w=np.hstack([self._w,alpha_random])\n",
        "\n",
        "          # print the shape everytime the loop runs\n",
        "          print(self._beta.shape,beta_random.shape)\n",
        "          self._beta = np.vstack([self._beta,beta_random])\n",
        "          H = self.sigmoid(X.dot(self._w))\n",
        "          \n",
        "          # pseudoinverse \n",
        "          H_pinv = np.linalg.pinv(H)\n",
        "          \n",
        "          # below step updates beta\n",
        "          self._beta = H_pinv.dot(Y)\n",
        "\n",
        "      # to provide the time taken for training \n",
        "      if display_time:\n",
        "          start = time.time()\n",
        "      H_pinv = np.linalg.pinv(H)\n",
        "      if display_time:\n",
        "          stop = time.time()\n",
        "          print(f'Train time: {stop-start}')\n",
        "\n",
        "      self._beta = H_pinv.dot(Y)\n",
        "\n",
        "  def __call__(self, X):\n",
        "      H = self._activation(X.dot(self._w) + self._bias)\n",
        "      return H.dot(self._beta)\n",
        "\n",
        "  # initialising a function to evaluate the loss and accuracy of the model\n",
        "  def compute_loss_acc(self, X, Y):\n",
        "      pred = self.predict(X)\n",
        "\n",
        "      # compute Loss\n",
        "      loss = self._loss(Y, pred)\n",
        "\n",
        "      # compute Accuracy\n",
        "      acc = np.sum(np.argmax(pred, axis=-1) == np.argmax(Y, axis=-1)) / len(Y)\n",
        "\n",
        "      return loss, acc"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AvpbhklWZx5"
      },
      "source": [
        "# Initialising model variables and declaring them\n",
        "classes = 10\n",
        "hidden_layers = 512\n",
        "\n",
        "# input lenght is 28**2 as it is 28x28 \n",
        "input_length = 28**2\n",
        "\n",
        "LMax=100"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqqKC1iVWcXV",
        "outputId": "120c7a01-f085-4606-8e43-efc1b185e5b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Loading MNIST Dataset using the library functionality of Keras\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Processing each image which is a 28*28 pixel into input vectors between 0 and 255 for training\n",
        "x_train = x_train.astype(np.float32) / 255.\n",
        "x_train = x_train.reshape(-1, input_length)\n",
        "\n",
        "# Processing each image which is a 28*28 pixel into input vectors between 0 and 255 for testing\n",
        "x_test = x_test.astype(np.float32) / 255.\n",
        "x_test = x_test.reshape(-1, input_length)\n",
        "\n",
        "# converting the value to categorical\n",
        "y_train = to_categorical(y_train, classes).astype(np.float32)\n",
        "y_test = to_categorical(y_test, classes).astype(np.float32)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bbsmNTmWezq"
      },
      "source": [
        "# create instance of our model\n",
        "model = IELM(\n",
        "    input_length,\n",
        "    hidden_layers,\n",
        "    classes\n",
        ")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZXV3XTUWhcX",
        "outputId": "d8b47c90-9592-4dbf-abcc-1bfad5422ebe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Train model and compute accuracy and loss\n",
        "model.fit(x_train, y_train, LMax, display_time=True)\n",
        "\n",
        "# computing time taken\n",
        "timeTaken_train = time.time()\n",
        "loss_train, accuracy_train = model.compute_loss_acc(x_train, y_train)\n",
        "final_timeTaken_train = time.time()\n",
        "print('training loss:', loss_train)\n",
        "print('training accuracy:', accuracy_train)\n",
        "print('Total Time require for Training is (in Seconds):', (final_timeTaken_train-timeTaken_train))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 10) (1, 10)\n",
            "(2, 10) (1, 10)\n",
            "(3, 10) (1, 10)\n",
            "(4, 10) (1, 10)\n",
            "(5, 10) (1, 10)\n",
            "(6, 10) (1, 10)\n",
            "(7, 10) (1, 10)\n",
            "(8, 10) (1, 10)\n",
            "(9, 10) (1, 10)\n",
            "(10, 10) (1, 10)\n",
            "(11, 10) (1, 10)\n",
            "(12, 10) (1, 10)\n",
            "(13, 10) (1, 10)\n",
            "(14, 10) (1, 10)\n",
            "(15, 10) (1, 10)\n",
            "(16, 10) (1, 10)\n",
            "(17, 10) (1, 10)\n",
            "(18, 10) (1, 10)\n",
            "(19, 10) (1, 10)\n",
            "(20, 10) (1, 10)\n",
            "(21, 10) (1, 10)\n",
            "(22, 10) (1, 10)\n",
            "(23, 10) (1, 10)\n",
            "(24, 10) (1, 10)\n",
            "(25, 10) (1, 10)\n",
            "(26, 10) (1, 10)\n",
            "(27, 10) (1, 10)\n",
            "(28, 10) (1, 10)\n",
            "(29, 10) (1, 10)\n",
            "(30, 10) (1, 10)\n",
            "(31, 10) (1, 10)\n",
            "(32, 10) (1, 10)\n",
            "(33, 10) (1, 10)\n",
            "(34, 10) (1, 10)\n",
            "(35, 10) (1, 10)\n",
            "(36, 10) (1, 10)\n",
            "(37, 10) (1, 10)\n",
            "(38, 10) (1, 10)\n",
            "(39, 10) (1, 10)\n",
            "(40, 10) (1, 10)\n",
            "(41, 10) (1, 10)\n",
            "(42, 10) (1, 10)\n",
            "(43, 10) (1, 10)\n",
            "(44, 10) (1, 10)\n",
            "(45, 10) (1, 10)\n",
            "(46, 10) (1, 10)\n",
            "(47, 10) (1, 10)\n",
            "(48, 10) (1, 10)\n",
            "(49, 10) (1, 10)\n",
            "(50, 10) (1, 10)\n",
            "(51, 10) (1, 10)\n",
            "(52, 10) (1, 10)\n",
            "(53, 10) (1, 10)\n",
            "(54, 10) (1, 10)\n",
            "(55, 10) (1, 10)\n",
            "(56, 10) (1, 10)\n",
            "(57, 10) (1, 10)\n",
            "(58, 10) (1, 10)\n",
            "(59, 10) (1, 10)\n",
            "(60, 10) (1, 10)\n",
            "(61, 10) (1, 10)\n",
            "(62, 10) (1, 10)\n",
            "(63, 10) (1, 10)\n",
            "(64, 10) (1, 10)\n",
            "(65, 10) (1, 10)\n",
            "(66, 10) (1, 10)\n",
            "(67, 10) (1, 10)\n",
            "(68, 10) (1, 10)\n",
            "(69, 10) (1, 10)\n",
            "(70, 10) (1, 10)\n",
            "(71, 10) (1, 10)\n",
            "(72, 10) (1, 10)\n",
            "(73, 10) (1, 10)\n",
            "(74, 10) (1, 10)\n",
            "(75, 10) (1, 10)\n",
            "(76, 10) (1, 10)\n",
            "(77, 10) (1, 10)\n",
            "(78, 10) (1, 10)\n",
            "(79, 10) (1, 10)\n",
            "(80, 10) (1, 10)\n",
            "(81, 10) (1, 10)\n",
            "(82, 10) (1, 10)\n",
            "(83, 10) (1, 10)\n",
            "(84, 10) (1, 10)\n",
            "(85, 10) (1, 10)\n",
            "(86, 10) (1, 10)\n",
            "(87, 10) (1, 10)\n",
            "(88, 10) (1, 10)\n",
            "(89, 10) (1, 10)\n",
            "(90, 10) (1, 10)\n",
            "(91, 10) (1, 10)\n",
            "(92, 10) (1, 10)\n",
            "(93, 10) (1, 10)\n",
            "(94, 10) (1, 10)\n",
            "(95, 10) (1, 10)\n",
            "(96, 10) (1, 10)\n",
            "(97, 10) (1, 10)\n",
            "(98, 10) (1, 10)\n",
            "Train time: 1.0628809928894043\n",
            "training loss: 0.0238692912344254\n",
            "training accuracy: 0.7970166666666667\n",
            "Total Time require for Training is (in Seconds): 0.8500585556030273\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXYTuD6YW3lI",
        "outputId": "7972294d-6be2-431d-bed2-e9d863c6780c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "#  Test model and compute accuracy and loss\n",
        "timeTaken_test = time.time()\n",
        "loss_test, accuracy_test = model.compute_loss_acc(x_test, y_test)\n",
        "final_timeTaken_test = time.time()\n",
        "print('Testing loss:', loss_test)\n",
        "print('Testing accuracy:', accuracy_test)\n",
        "print('Total Time required for Testing is (in seconds):', (final_timeTaken_test-timeTaken_test))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing loss: 0.023630283982512396\n",
            "Testing accuracy: 0.8035\n",
            "Total Time required for Testing is (in seconds): 0.1660621166229248\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}